from llm_utils.vector_store import load_vectorstore
from llm_utils.prompt_template import get_prompt
from llm_utils.gpt_client import generate_response
from llm_utils.retriever import get_multiquery_retriever
from llm_utils.normalizer import normalize_question

context_boilerplate = """
ì´ ì±—ë´‡ì€ ìˆ˜ì›ëŒ€í•™êµì˜ ì§€ëŠ¥í˜•SWìœµí•©ëŒ€í•™ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•©ë‹ˆë‹¤.

ì§€ëŠ¥í˜•SWìœµí•©ëŒ€í•™ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- ì»´í“¨í„°í•™ë¶€ â†’ ì»´í“¨í„°SWí•™ê³¼, ë¯¸ë””ì–´SWí•™ê³¼
- ì •ë³´í†µì‹ í•™ë¶€ â†’ ì •ë³´í†µì‹ í•™ê³¼, ì •ë³´ë³´í˜¸í•™ê³¼
- ë°ì´í„°ê³¼í•™ë¶€ (ë‹¨ì¼)
- í´ë¼ìš°ë“œìœµë³µí•©ì „ê³µ (ë‹¨ì¼)
ì¦‰, "ì»´í“¨í„°SWí•™ê³¼"ëŠ” "ì»´í“¨í„°í•™ë¶€"ì— í¬í•¨ë©ë‹ˆë‹¤.

[ì£¼ìš” ì¼ì • ì •ë³´]
- 1í•™ê¸° ì¢…ê°•ì¼: 6ì›” 25ì¼
- 2í•™ê¸° ì¢…ê°•ì¼: 12ì›” 15ì¼

í•˜ê³„ë°©í•™ì€ 1í•™ê¸° ì¢…ê°• í›„ ì‹œì‘ë˜ë©°, ë™ê³„ë°©í•™ì€ 2í•™ê¸° ì¢…ê°• í›„ ì‹œì‘ë©ë‹ˆë‹¤.
"""



def format_docs_with_boilerplate(docs):
    context_from_docs = "\n\n".join([d.page_content for d in docs])
    return context_boilerplate + "\n\n" + context_from_docs

def ask_with_rag(user_input):
    normalized = normalize_question(user_input)
    print(f"ğŸ“Œ ì •í˜•í™”ëœ ì§ˆë¬¸: {normalized}")
    vectorstore = load_vectorstore()
    retriever = get_multiquery_retriever(vectorstore)
    retrieved_docs = retriever.invoke(normalized)
    print("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:", len(retrieved_docs))
    
    full_context = format_docs_with_boilerplate(retrieved_docs)
    prompt = get_prompt()
    messages = prompt.format_messages(context=full_context, question=user_input)
    print("messages:", user_input)
    return generate_response(messages).content.strip() 