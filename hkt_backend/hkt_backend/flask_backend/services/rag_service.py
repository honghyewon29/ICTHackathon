# rag_service.py
from llm_utils.vector_store import load_vectorstore
from llm_utils.prompt_template import get_prompt
from llm_utils.gpt_client import generate_response
from llm_utils.retriever import get_multiquery_retriever
from llm_utils.normalizer import normalize_question
# from googletrans import Translator
from deep_translator import GoogleTranslator
import langid

def detect_language(text):
    lang, _ = langid.classify(text)
    return lang


context_boilerplate = """
ì´ ì±—ë´‡ì€ ìˆ˜ì›ëŒ€í•™êµì˜ ì§€ëŠ¥í˜•SWìœµí•©ëŒ€í•™ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•©ë‹ˆë‹¤.

ì§€ëŠ¥í˜•SWìœµí•©ëŒ€í•™ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- ì»´í“¨í„°í•™ë¶€ â†’ ì»´í“¨í„°SWí•™ê³¼, ë¯¸ë””ì–´SWí•™ê³¼
- ì •ë³´í†µì‹ í•™ë¶€ â†’ ì •ë³´í†µì‹ í•™ê³¼, ì •ë³´ë³´í˜¸í•™ê³¼
- ë°ì´í„°ê³¼í•™ë¶€ (ë‹¨ì¼)
- í´ë¼ìš°ë“œìœµë³µí•©ì „ê³µ (ë‹¨ì¼)
ì¦‰, "ì»´í“¨í„°SWí•™ê³¼"ëŠ” "ì»´í“¨í„°í•™ë¶€"ì— í¬í•¨ë©ë‹ˆë‹¤.

ì»´í“¨í„°sw í•™ê³¼ êµìˆ˜ì§„ ì •ë³´

ì¥ì„±íƒœ êµìˆ˜ëŠ” ì»´í“¨í„°SWí•™ê³¼ ì†Œì†ìœ¼ë¡œ, ì „ê³µì€ ì»´í“¨í„°êµ¬ì¡°, ì°¨ì„¸ëŒ€ Mobile Embedded System, ë³´ì•ˆê°ì‹œ ê¸°ìˆ ì´ë©°, ì´ë©”ì¼ì€ stjhang@suwon.ac.kr, ì—°êµ¬ì‹¤ì€ ICT ìœµí•©ëŒ€í•™ 510í˜¸, ì—°ë½ì²˜ëŠ” 031-220-2126ì…ë‹ˆë‹¤.

í•œì„±ì¼ êµìˆ˜ëŠ” ì»´í“¨í„°SWí•™ê³¼ ì†Œì†ì´ë©°, ì „ê³µì€ Applied Machine Learningì…ë‹ˆë‹¤. ì´ë©”ì¼ì€ seongil.han@suwon.ac.kr, ì—°êµ¬ì‹¤ì€ ICT ìœµí•©ëŒ€í•™ 521í˜¸, ì—°ë½ì²˜ëŠ” 031-229-8218ì…ë‹ˆë‹¤.

ì¤€ì›¨ì´í‘¸ êµìˆ˜ëŠ” ì»´í“¨í„°SWí•™ê³¼ ì†Œì†ì´ë©°, ì „ê³µê³¼ ì—°ë½ì²˜ ì •ë³´ëŠ” ì—†ìœ¼ë©°, ì´ë©”ì¼ë„ ì—†ìŠµë‹ˆë‹¤. ì—°êµ¬ì‹¤ì€ ITëŒ€í•™ 405í˜¸ì…ë‹ˆë‹¤.

ê¹€ì¥ì˜ êµìˆ˜ëŠ” ì»´í“¨í„°SWí•™ê³¼ ì†Œì†ì´ë©°, ì „ê³µì€ ë¹…ë°ì´í„°, ë„¤íŠ¸ì›Œí¬, ì¸ê³µì§€ëŠ¥, ë³´ì•ˆì…ë‹ˆë‹¤. ì´ë©”ì¼ì€ jykim77@suwon.ac.kr, ì—°êµ¬ì‹¤ì€ ì§€ëŠ¥í˜•SWìœµí•©ëŒ€í•™ 522í˜¸, ì—°ë½ì²˜ëŠ” 031-229-8345ì…ë‹ˆë‹¤.

êµ¬ì°½ì§„ êµìˆ˜ëŠ” ì»´í“¨í„°SWí•™ê³¼ ì†Œì†ì´ë©°, ì „ê³µì€ ìš´ì˜ì²´ì œì™€ ì •ë³´ë³´í˜¸ì…ë‹ˆë‹¤. ì´ë©”ì¼ì€ ycjkoo@suwon.ac.kr, ì—°êµ¬ì‹¤ì€ ë¯¸ë˜í˜ì‹ ê´€ 712í˜¸, ì—°ë½ì²˜ëŠ” 031-229-8595ì…ë‹ˆë‹¤.

í—ˆì„±ë¯¼ êµìˆ˜ëŠ” ì»´í“¨í„°SWí•™ê³¼ ì†Œì†ì´ë©°, ì „ê³µ, ì´ë©”ì¼, ì—°êµ¬ì‹¤, ì—°ë½ì²˜ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.


í•˜ê³„ë°©í•™ì€ 1í•™ê¸° ì¢…ê°• í›„ ì‹œì‘ë˜ë©°, ë™ê³„ë°©í•™ì€ 2í•™ê¸° ì¢…ê°• í›„ ì‹œì‘ë©ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì…”í‹€ë²„ìŠ¤ì— ëŒ€í•œ ë‚´ìš©ì´ë©´, ì…”í‹€ë²„ìŠ¤ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ì…”í‹€ì´ ì•„ë‹Œ ëŒ€ì¤‘êµí†µ(ì‹œë‚´/ê´‘ì—­/ë§ˆì„ë²„ìŠ¤ ë“±) ê´€ë ¨ì´ë©´, ëŒ€ì¤‘êµí†µ ë²„ìŠ¤ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
ì–´ë–¤ ì¢…ë¥˜ì¸ì§€ êµ¬ë¶„ì´ ì–´ë µë‹¤ë©´, ë¨¼ì € ì–´ë–¤ ì •ë³´ë¥¼ ì›í•˜ëŠ”ì§€ ë¬¼ì–´ë³´ì„¸ìš”.
s
"""



def format_docs_with_boilerplate(docs):
    context_from_docs = "\n\n".join([d.page_content for d in docs])
    return context_boilerplate + "\n\n" + context_from_docs

translator = GoogleTranslator()
from openai import OpenAI
import langid
from config import OPENAI_API_KEY

client = OpenAI(api_key=OPENAI_API_KEY)  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ë©´ ë” ì•ˆì „í•´

def detect_language_with_gpt(text):
    system_prompt = (
        "ë„ˆëŠ” ì–¸ì–´ ê°ì§€ ì „ë¬¸ê°€ì•¼. ë‹¤ìŒ ë¬¸ì¥ì˜ ì–¸ì–´ë¥¼ ISO 639-1 ì½”ë“œ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´. "
        "ì˜ˆ: í•œêµ­ì–´ëŠ” 'ko', ì˜ì–´ëŠ” 'en', ì¼ë³¸ì–´ëŠ” 'ja', ì•„ëì–´ëŠ” 'ar'"
    )
    user_prompt = f"ë¬¸ì¥: {text}"

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0,
            max_tokens=5
        )
        lang = response.choices[0].message.content.strip().lower()
        print(f"ğŸŒ GPT ê°ì§€ ì–¸ì–´: {lang}")
        return lang
    except Exception as e:
        print(f"âŒ GPT ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨: {e} â†’ langid fallback")
        lang, _ = langid.classify(text)
        return lang

def ask_with_rag(user_input):
    # 1. ì–¸ì–´ ê°ì§€ (ë‹¨ì–´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì˜ì–´ë¡œ ê°„ì£¼)
    lang = detect_language_with_gpt(user_input)
    print(f"ğŸŒ ê°ì§€ëœ ì–¸ì–´: {lang}")

    # 2. ê²€ìƒ‰ìš© ë²ˆì—­ (ë¹„í•œêµ­ì–´ â†’ í•œêµ­ì–´)
    if lang != "ko":
        translated = translator.translate(user_input, src=lang, dest="ko")
        print(f"ğŸ” ë²ˆì—­ëœ ì§ˆë¬¸ (ê²€ìƒ‰ìš©): {translated}")
    else:
        translated = user_input

    # 3. ì •í˜•í™”
    normalized = normalize_question(translated)
    print(f"ğŸ“Œ ì •í˜•í™”ëœ ì§ˆë¬¸: {normalized}")

    # 4. ê²€ìƒ‰
    vectorstore = load_vectorstore()
    retriever = get_multiquery_retriever(vectorstore)
    retrieved_docs = retriever.invoke(normalized)

    # 5. context ìƒì„±
    from services.rag_service import format_docs_with_boilerplate  # ì¬ê·€ import í”¼í•˜ê¸° ìœ„í•´ ì´ë ‡ê²Œ ê°€ëŠ¥
    full_context = format_docs_with_boilerplate(retrieved_docs)

    # 6. í”„ë¡¬í”„íŠ¸ ìƒì„± (ì§ˆë¬¸ì€ ì›ë¬¸, ì–¸ì–´ëŠ” ê°ì§€ëœ ê°’)
    prompt = get_prompt()
    messages = prompt.format_messages(
        context=full_context,
        question=user_input,
        language=lang
    )

    # 7. ì‘ë‹µ ìƒì„±
    response = generate_response(messages)
    return response.content.strip()